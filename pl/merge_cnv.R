library(plyr)
library(dplyr)
library(tidyr)
library(stats)
library(zoo)

# a rough filter by eye to remove high variance regions
# drop if variance of median CN over 9 windows is > 0.3 or a window's median is > 3 
median.win <- 10
median.var.max <- 0.3
median.max <- 3

cmd.args <- commandArgs(trailingOnly = TRUE)
# cmd.args <- c('d:/mccarroll/cnv_seg.12.500/windows.vcf.gz.txt','0.80','20','500','d:/mccarroll//cnv_seg.12.500/sites_cnv_segs.txt')
cnv.geno.fn <- cmd.args[1]
pass.thresh <- as.numeric(cmd.args[2])  # e.g. .80. Drop sites entirely if less than pass.thresh % of samples have FT ("per-sample genotype filter") of PASS
qual.thresh <- as.numeric(cmd.args[3]) 		# CNQ phred-style threshold, e.g. 20
max.join <- as.numeric(cmd.args[4])  # size (in nt) of runs to span if flanking calls are the same, e.g. 2000
cnv.seg.fn <- cmd.args[5]   # a delimited "site" output file of putative regions
extend.qual.thresh <- as.numeric(cmd.args[6])

#################################################################################################
# PROCESS genotyper as above
# file is generated by vcf2tab and is of form:
# BIN CHR START   END PASS X09C81182 X09C81377 ... X09C81182_CNQ X09C81377_CNQ
cat(sprintf("Reading %s\n", cnv.geno.fn)) 
cnv.geno <- read.table(cnv.geno.fn, stringsAsFactors = FALSE, header=TRUE, check.names = FALSE)

# there are 5 site info columns followed by calls and quality per sample
sample.count <- (ncol(cnv.geno)-5)/2
cat(sprintf("Read %s samples at %s sites\n", sample.count, nrow(cnv.geno)))

# set the genotype values as numeric
for (s in 6:(5+sample.count)) {
  cnv.geno[,s] <- as.numeric(cnv.geno[,s])
}

low.qual <- rep(0,sample.count)
cnq.cols <- (ncol(cnv.geno)-sample.count+1):ncol(cnv.geno)
cat(sprintf("Setting CNQ=0 for all samples of low call rate (<%s) (%s sites)\n", pass.thresh,
            length(which(cnv.geno$PASS/sample.count < pass.thresh))))
cnv.geno <- cnv.geno[cnv.geno$PASS/sample.count > pass.thresh,]
cnv.geno[cnv.geno$PASS/sample.count < pass.thresh,cnq.cols] <- low.qual

cat("Computing median per site\n")
cnv.geno$site.median = apply(cnv.geno[,6:(5+sample.count)], 1, median, na.rm=TRUE)

cat(sprintf("Computing variance of median on rolling windows of length %s\n",median.win))
cnv.geno$site.median.var <- c(rep(0,median.win/2),
                              rollapply(cnv.geno$site.median, width=median.win, var, na.rm=TRUE),
                              rep(0,median.win/2-1))
cat(sprintf("Setting CNQ=0 for all samples with variance > %s (%s sites)\n", median.var.max, 
            sum(cnv.geno$site.median.var>median.var.max, na.rm=TRUE)))
cnv.geno[cnv.geno$site.median.var > median.var.max,cnq.cols] <- low.qual

cat(sprintf("Setting CNQ=0 for all samples with median CN > %s (%s sites)\n", median.max, sum(cnv.geno$site.median > median.max, na.rm=TRUE)))
cnv.geno[is.na(cnv.geno$site.median) | cnv.geno$site.median > median.max, cnq.cols] <- low.qual

# evaluate as rows and recover base positions later
pos.map <- cnv.geno[,1:4]
pos.map$CENTER <- floor(pos.map$START + (pos.map$END-pos.map$START)/2)
pos.map$BIN.SIZE <- c(pos.map$CENTER[2:nrow(pos.map)] - pos.map$CENTER[1:(nrow(pos.map)-1)],0)

# replace calls with NA if below quality threshold
cat(sprintf("Setting calls with CNQ <= %s to NA\n", qual.thresh))
for (i in 6:(6+sample.count-1)) {
  cnv.geno[,i] <- ifelse(cnv.geno[,i+sample.count]>qual.thresh,cnv.geno[,i],NA)
}
NA.calls <- sum(is.na(cnv.geno[,6:(5+sample.count)]))
cat(sprintf("Removed %s/%s = %s%% calls\n", NA.calls, sample.count*nrow(cnv.geno), NA.calls/sample.count/nrow(cnv.geno)*100))

# generate consecutive runs per sample
t1.fn <- tempfile("cnv")
t1.conn <- file(t1.fn,"w")
cat(".id,cn,start,end\n",file=t1.conn)
cat(sprintf("Merging runs of same CN in temp file %s\n",t1.fn))
for (s in 6:(5+sample.count)) {
  copyNumber <- cnv.geno[,s]
  sample.name <- colnames(cnv.geno)[s]
  cat(sprintf("Processing %s:%s\n",s,sample.name))

  pos.i <- 1
  for (i in pos.i+1:(length(copyNumber)-1)) {
    if (!(identical(copyNumber[i],copyNumber[pos.i]))) {
      # a change in the CNV, so add this cnv and reseti<- pos.i
      cat(paste(sample.name,copyNumber[pos.i],pos.i, i-1,sep=','),"\n",file=t1.conn)
      pos.i <- i
    }
  }
  cat(paste(sample.name,cn=copyNumber[pos.i],pos.i, end=length(copyNumber),sep=','),"\n",file=t1.conn)
}
close(t1.conn)
cat(sprintf("Loading %s into cn.segs\n",t1.fn))
cn.segs <- read.csv(t1.fn, as.is=TRUE, check.names=FALSE)
cat(sprintf("Read %s rows\n",nrow(cn.segs)))

# add actual nt positions
cn.segs$chr <- pos.map[cn.segs$start,'CHR']
cn.segs$start.map <- pos.map[cn.segs$start,'CENTER']
cn.segs$end.map <- pos.map[cn.segs$end,'CENTER']+pos.map[cn.segs$end,'BIN.SIZE']
cn.segs$copy.number <- addNA(as.factor(cn.segs$cn))


# merge small calls between common CN
cat(sprintf("Joining segments across small (%s) segments\n", max.join))
new.extents <-
  ddply(cn.segs, .(.id), function(cs) {
    cat(cs$.id[1],"\n")
    
    new.adj <- rep(NA,nrow(cs))
    i <- i2 <- 1  # i is the left-most extent and i2 is the right-most extent of the same cn
    cn.i <- cs$copy.number[i]
    end.map.i <- cs$end.map[i]
    j <- i+1
    while (j <= nrow(cs)) {
      if (cs$start.map[j]-end.map.i > max.join) {
        new.adj[i] <- i2
        i <- i2+1
        i2 <- i
        cn.i <- cs$copy.number[i]
        end.map.i <- cs$end.map[i]
        j <- i
      } else if (cs$copy.number[j]==cn.i) {
        end.map.i <- cs$end.map[j] # new rightmost position of current copy.number extent
        i2 <- j
      }
      j <- j + 1
    }
    
    # The start's are the indices of cs, which are the non-NA elements of new.adj
    start.i <- which(!is.na(new.adj))
    
    # The end's are the indices of cs corresponding to the adjacent contig or the end of a thread if i==new.adj[i]
    end.i <- new.adj[start.i]
    
    cn <- cs$cn[start.i]
    chr <- cs$chr[start.i]
    
    start.map <- cs$start.map[start.i]
    end.map <- cs$end.map[end.i]
    
    return(data.frame(cn, start.i, end.i, chr, start.map, end.map))
  })

# use new.extents to dereference cn.segs and create a new (condensed) set
cn.segs.merged <- mutate(new.extents,
                         copy.number=addNA(as.factor(cn)),
                         len=end.map-start.map,
                         seg=sprintf("SEG_%s_%s_%s", chr, start.map, end.map))
cn.segs.merged$len2 <- c(cn.segs.merged$start.map[2:nrow(cn.segs.merged)]-cn.segs.merged$start.map[1:(nrow(cn.segs.merged)-1)],0)
cn.segs.merged$gap <- c(cn.segs.merged$start.map[2:nrow(cn.segs.merged)]-cn.segs.merged$end.map[1:(nrow(cn.segs.merged)-1)],0)
# ggplot(subset(cn.segs.merged, copy.number != '2'), aes(y=.id, yend=.id, color=copy.number, x=start.map, xend=end.map)) + geom_segment(size=4)

# generate sites to run through org.broadinstitute.sv.apps.ProfileGenotyper
test.segs <- arrange(unique(cn.segs.merged[!is.na(cn.segs.merged$cn),c('seg','chr','start.map','end.map')]),start.map)
cat(sprintf("Writing %s CNV segments to %s\n",nrow(test.segs), cnv.seg.fn))
write.table(test.segs, file=cnv.seg.fn, sep="\t", row.names=FALSE, col.names=FALSE, quote=FALSE )

cn.segs.merged <- mutate(cn.segs.merged, label=sprintf("%s_%s",seg,.id))

# save for debugging and later processing
cat(sprintf("Writing R objects to %s*.Rdata\n",cnv.seg.fn))
save(cn.segs, file=sprintf("%s.cs.Rdata",cnv.seg.fn))
save(cn.segs.merged, file=sprintf("%s.csm.Rdata",cnv.seg.fn))
write.table(select(cn.segs.merged, .id, seg, chr, start.map, end.map, copy.number), file=sprintf("%s.tbl",cnv.seg.fn), sep="\t", row.names=FALSE, col.names=FALSE, quote=FALSE)
save(cnv.geno, file=sprintf("%s.cg.Rdata",cnv.seg.fn))

cn <- cnv.geno[,c(2:4,6:(5+sample.count))]
cn2 <- gather(cn,sample,cn,-CHR,-START,-END)

cnq <- cnv.geno[,c(2:4,(6+sample.count):(ncol(cnv.geno)-2))]
cnq2 <- gather(cnq,sample,cnq,-CHR,-START,-END)

cnv.geno <- cbind(cn2, cnq=cnq2$cnq)
rm(cn,cn2,cnq,cnq2)

cnv.geno.exploded.fn <- sprintf("%s.cnvgeno.txt",cnv.seg.fn)
cat(sprintf("Writing %s exploded genotype calls to %s\n",nrow(cnv.geno),cnv.geno.exploded.fn))
colnames(cnv.geno) <- c('chr','start','end','sample','cn','cnq')
write.table(cnv.geno, file=cnv.geno.exploded.fn, quote=FALSE, sep="\t", row.names=FALSE)

write.table(cn.segs.merged[!is.na(cn.segs.merged$cn),c('chr','start.map','end.map','label','cn')], file=sprintf("%s.mrg.bed",cnv.seg.fn), sep="\t", col.names=FALSE, quote=FALSE, row.names=FALSE)

# ###############################################################################################################
# 
# # create a set of CNVs that extend their boundaries as follows:
# #  - skip an NA extent, if exists
# #  - if the adjacent is CN=2, then choose its most proximal high CNQ position
# #  -    else choose the midpoint
# 
# # A lookup of sample ID to column, so it's easy to dereference cnv.geno.
# colidx <- as.list(1:ncol(cnv.geno))
# names(colidx) <- colnames(cnv.geno)
# 
# HI.QUAL.THRESH <- extend.qual.thresh
# 
# new.cn.segs.merged <- ddply(cn.segs.merged, .(.id), function(df) {
#   cat(df$.id[1],"\n")
#   cn.segs.sub <- cn.segs %>% subset(.id==df$.id[1])
#   new.bounds <- ldply(1:nrow(df), function(idx) {
#     if (is.na(df$cn[idx]) || df$cn[idx] == 2) { # skip if wildtype CN=2
#       new.start.map <- df$start.map[idx]
#       new.end.map <- df$end.map[idx]
#     } else {
#       prev.extent <- idx - 1
#       prev.extent <- ifelse(prev.extent>1 && is.na(df$cn[prev.extent]), prev.extent - 1, prev.extent)
#       if (prev.extent > 0 && !is.na(df$cn[prev.extent])) {
#         prev.extent.cnv.geno.idx <- cn.segs.sub$end[df$end.i[prev.extent]]
#         offsets <- prev.extent.cnv.geno.idx - 0:10
#         cnqs <- as.numeric(cnv.geno[offsets,paste0(df$.id[prev.extent],'_CNQ')])
#         offsets.hiqual <- offsets[cnqs >= HI.QUAL.THRESH]
#         if (length(offsets.hiqual)>0) {
#           # the right-most high quality bin. I'm currently choosing the mid-point of the high quality bin
#           # but might be better to choose the edge of the bin.
#           new.start.map <- pos.map$CENTER[offsets.hiqual[1]]
#         } else {
#           # half-way between current start and end of previous extent
#           new.start.map <- df$end.map[prev.extent] + (df$start.map[idx]-df$end.map[prev.extent])/2
#         }
#       } else {
#         new.start.map <- df$start.map[idx] # no change
#       }
#       
#       next.extent <- idx + 1
#       if (next.extent < nrow(df) && is.na(df$cn[next.extent])) { next.extent <- next.extent + 1 }
#       if (next.extent <= nrow(df) && !is.na(df$cn[next.extent])) {
#         next.extent.cnv.geno.idx <- cn.segs.sub$start[df$start.i[next.extent]]
#         offsets <- next.extent.cnv.geno.idx + 0:10
#         cnqs <- as.numeric(cnv.geno[offsets,paste0(df$.id[next.extent],'_CNQ')])
#         offsets.hiqual <- offsets[cnqs >= HI.QUAL.THRESH]
#         if (length(offsets.hiqual)>0) {
#           # the left-most high quality bin. I'm currently choosing the mid-point of the high quality bin
#           # but might be better to choose the edge of the bin.
#           new.end.map <- pos.map$CENTER[offsets.hiqual[1]]
#         } else {
#           # half-way between current end and start of next extent
#           new.end.map <- df$end.map[idx] + (df$start.map[next.extent]-df$end.map[idx])/2
#         }
#       } else {
#         new.end.map <- df$end.map[idx] # no change
#       }
#     }
# #    cat(df$.id[idx],df$cn[idx],new.start.map,df$start.map[idx],df$end.map[idx],new.end.map,"\n")
#     return(data.frame(.id=df$.id[idx], new.start.map=new.start.map, new.end.map=new.end.map,seg=df$seg[idx]))
#   })
#   return(new.bounds)
# })
# 
# # Create a new version of df and save it. Use same variable name so we can swap between the two
# # by just choosing a different file name.
# old.csm <- cn.segs.merged
# cn.segs.merged$start.map <- as.integer(new.cn.segs.merged$new.start.map)
# cn.segs.merged$end.map <- as.integer(new.cn.segs.merged$new.end.map)
# 
# save(cn.segs.merged, file=sprintf("%s.ncsm.Rdata",cnv.seg.fn))
# write.table(select(cn.segs.merged, .id, seg, chr, start.map, end.map, copy.number), file=sprintf("%s.ext.tbl",cnv.seg.fn), sep="\t", row.names=FALSE, col.names=FALSE, quote=FALSE)
# 
# write.table(cn.segs.merged[!is.na(cn.segs.merged$cn),c('chr','start.map','end.map','label','cn')], file=sprintf("%s.nmrg.bed",cnv.seg.fn), sep="\t", col.names=FALSE, quote=FALSE, row.names=FALSE)
# 
# ############################################################################################################
# # create a set of CNVs that extend their boundaries as follows:
# #  - skip an NA extent, if exists
# #  - Retrieve the profiles for a window straddling boundary.
# #  - Compute max likelihood transition location
# 
# # A lookup of sample ID to column, so it's easy to dereference cnv.geno.
# cn.segs.merged <- old.csm
# 
# ## DEVEL
# # cmd.args <- c('d:/mccarroll/cnv_seg.12.500/windows.vcf.gz.txt','0.80','20','500','d:/mccarroll//cnv_seg.12.500/sites_cnv_segs.txt')
# # cmd.args <- c('d:/mccarroll/cnv_seg.B12.L500.Q13/windows.vcf.gz.txt', 0.80, 13, 500, 'd:/mccarroll/cnv_seg.B12.L500.Q13/sites_cnv_segs.txt')
# # cnv.seg.fn <- cmd.args[5]   # a delimited "site" output file of putative regions
# # load(sprintf("%s.cs.Rdata",cnv.seg.fn)) # cn.segs
# # load(sprintf("%s.csm.Rdata",cnv.seg.fn)) # cn.segs.merged
# # load(sprintf("%s.cg.Rdata",cnv.seg.fn)) # cnv.geno
# # 
# # cn.segs.merged <- as.tbl(cn.segs.merged)
# # cn.segs <- as.tbl(cn.segs)
# # cnv.geno <- as.tbl(cnv.geno)
# # pos.map <- cnv.geno[,1:4]
# # pos.map$CENTER <- floor(pos.map$START + (pos.map$END-pos.map$START)/2)
# # pos.map$BIN.SIZE <- c(pos.map$CENTER[2:nrow(pos.map)] - pos.map$CENTER[1:(nrow(pos.map)-1)],0)
# 
# db <- src_postgres(dbname='seq', host='localhost', port=5432, user='postgres')
# library(RPostgreSQL)
# ###
# region.offset <- 1000
# 
# bin.map <- dbGetQuery(db$con, "select * from profile_segment")
# rownames(bin.map) <- bin.map$bin
# 
# win.size <- first(which(cumsum(bin.map$elength)>=region.offset)) # set window size (in bins) to the size of region.offset
# 
# half.win <- win.size / 2 # each window is divided into 2 equal sides for cnA and cnB
# 
# # NOT USED
# # precompute all possible null models: all CN=a, but observe CN=b
# # each is a half.win
# nulls <- 
#   lapply(c(0:3), function(a) {
#     lapply(c(0.1,1:3), function(b) {
#       dpois(a*half.win,b*half.win)
#     })
#   })
# nulls <- matrix(unlist(nulls),ncol=4, byrow=TRUE)  # nulls[a,b] => CN=A, but observe CN=b
# 
# # surprisingly, the first or second method is 100X faster than the third
# # x <- pos-region.offset
# # ptm <- proc.time()
# # for (i in 1:1000) {
# #   idx <- min(which(bin.map$end_pos >= x))
# # #  print(max(which(bin.map$start_pos <= pos+region.offset)))
# # }
# # print(proc.time() - ptm)
# # 
# # ptm <- proc.time()
# # for (i in 1:1000) {
# #   idx <- which.min(bin.map$end_pos < x)
# # }
# # print(proc.time() - ptm)
# # 
# # ptm <- proc.time()
# # for (i in 1:100) {
# #   idx <- 1
# #   while (bin.map$start_pos[idx] < x) { idx <- idx + 1 }
# # }
# # print(proc.time() - ptm)
# 
# # # retrieve the profile counts for the region of interest
# # # compute the likelihood of a transition from cnA to cnB for sliding windows in the region
# # ml.transition <- function(sample, pos, region.offset, cnA, cnB) {
# #   
# #   # querying the database for each breakpoint is just too slow.
# #   # For the join, postgres refuses to do the right thing
# #   # I can force the right query using a temporary table, but the queries are still relatively slow.
# #   # There's no prepared statement, so each query must be interpolated with sprintf and then compiled
# #   # by the backend. 
# #   #  postgresqlExecStatement(db$con, "select bin, elength, end_pos into temporary a from profile_segment ps where chrom='20' and start_pos < $1 and end_pos > $2", list(pos+region.offset, pos-region.offset))
# #   #  res <- postgresqlExecStatement(db$con,"select * from profile_counts pc, a ps 
# #   #                                        where pc.bin=ps.bin AND pc.sample=$1", sample)
# #   #  profiles <- postgresqlFetch(res)
# #   #  postgresqlCloseResult(res)
# #   # dbGetQuery(db$con, sprintf("select bin, elength, end_pos into temporary a from profile_segment ps where chrom='20' and start_pos < %f+%f and end_pos > %f-%f", pos, region.offset, pos, region.offset))
# #   # profiles <- dbGetQuery(db$con,sprintf("select * from profile_counts pc, a ps 
# #   #                                       where pc.bin=ps.bin AND pc.sample='%s'", sample))
# #   #  dbGetQuery(db$con, "select bin, elength, end_pos into temporary a from profile_segment ps where chrom='20' and start_pos < 29541902+1000 and end_pos > 29542431-1000")
# #   #  profiles <- dbGetQuery(db$con,"select * from profile_counts pc, a ps 
# #   #                                        where pc.bin=ps.bin AND pc.sample='08C79660'")
# #   #  dbGetQuery(db$con, "drop table a")
# #   
# #   
# #   bin.map$idx <- 1:nrow(bin.map)
# #   bins <- subset(bin.map, chrom=='20' & start_pos < pos+region.offset & end_pos > pos-region.offset)
# #   
# #   profiles <- subset(all.profiles, bin %in% bins$bin)
# #   
# #   win.size <- first(which(cumsum(bins$elength)>=region.offset)) # set window size (in bins) to the size of region.offset
# #   
# #   half.win <- win.size / 2 # each window is divided into 2 equal sides for cnA and cnB
# #   
# #   # set cn=0.1 if cn=0
# #   cnA <- ifelse(cnA==0,0.1,cnA)
# #   cnB <- ifelse(cnB==0,0.1,cnB)
# #   
# #   expA <- rollapply(profiles$expected[1:(nrow(profiles)-half.win)]*cnA, half.win, sum)
# #   expB <- rollapply(profiles$expected[(1+half.win):nrow(profiles)]*cnB, half.win, sum)
# #   obsvA <- rollapply(profiles$observed[1:(nrow(profiles)-half.win)], half.win, sum)
# #   obsvB <- rollapply(profiles$observed[(1+half.win):nrow(profiles)], half.win, sum)
# #   pA <- dpois(obsvA, expA)
# #   pB <- dpois(obsvB, expB)
# #   jp <- pA * pB    # joint likelihood
# #   jp.norm <- jp / sum(jp)  # normalized
# #   
# #   best.pos <- bin.map[profiles$bin[which.max(jp.norm)+half.win],'end_pos']
# #   #  cat(sprintf("%s:%f => %f (%f) (%.4f..%.4f)\n", sample, pos, best.pos, best.pos-pos, min(-log(jp.norm)), max(-log(jp.norm))))
# #   return(best.pos)
# # }  
# # 
# 
# 
# 
# new.cn.segs.merged <- ddply(cn.segs.merged, .(.id), function(df) {
#   # df <- cn.segs.merged[cn.segs.merged$.id=='08C79660',]
#   sample <- df$.id[1]
#   cat(sample,"\n")
#   all.profiles <- dbGetQuery(db$con, sprintf("select * from profile_counts where sample='%s'", sample))
#   exp.cn <- llply(c(0.1,1:3), function(cn) {
#     rollapply(all.profiles$expected[1:(nrow(all.profiles)-half.win)]*cn, half.win, sum)
#   })
#   obs.cn <- rollapply(all.profiles$observed[1:(nrow(all.profiles)-half.win)], half.win, sum)
#   pois.cn <- llply(1:4, function(cn) {
#     dpois(obs.cn, exp.cn[[cn]])
#   })
# 
#   ml.transition2 <- function(sample, pos1, pos2, cnA, cnB) {
#     cnA <- ifelse(cnA > 3, 4, cnA+1)  # map CN to index into pois.cn. CN > 3 => CN:=3
#     cnB <- ifelse(cnB > 3, 4, cnB+1)
#     binL <- min(which(bin.map$end_pos >= pos1-region.offset))
#     binR <- max(which(bin.map$start_pos <= pos2+region.offset))
#     bin.count <- binR - binL + 1
#     pA <- pois.cn[[cnA]][binL:(binL+bin.count-win.size)] 
#     pB <- pois.cn[[cnB]][(binL+half.win):(binL+bin.count-half.win)]
#     jp <- pA * pB    # joint likelihood
#     jp.norm <- jp / sum(jp)  # normalized
#     best.pos <- bin.map[binL+which.max(jp.norm)+half.win-1,'start_pos']
#     #  cat(sprintf("%s:%.0f => %.0f (%.0f) (%.4f..%.4f)\n", sample, pos, best.pos, best.pos-pos, min(-log(jp.norm)), max(-log(jp.norm))))
#     if (length(best.pos)>0) {
#       return(best.pos)
#     } else {
#       return(pos1 + (pos2-pos1)/2)  # all NA or NaN
#     }
#   }
# 
#   new.bounds <- ldply(1:nrow(df), function(idx) {
#     if (idx %% 50 == 1) { cat(sprintf("%d / %d\n", idx, nrow(df))) }
#     if (is.na(df$cn[idx]) || df$cn[idx] == 2 || df$len[idx] < 2*region.offset) { # skip if wildtype CN=2
#       new.start.map <- df$start.map[idx]
#       new.end.map <- df$end.map[idx]
#     } else {
#       prev.extent <- idx - 1
#       # skip over an intervening NA if it's not too big
#       prev.extent <- ifelse(prev.extent>1 && is.na(df$cn[prev.extent]) && df$end.map[prev.extent]-df$start.map[prev.extent] < max.join*4, prev.extent - 1, prev.extent)
#       if (prev.extent > 0 && !is.na(df$cn[prev.extent])) {
#         # begin search at the half-way point between current and previous
#         new.start.map <- ml.transition2(sample, df$end.map[prev.extent], df$start.map[idx], df$cn[prev.extent], df$cn[idx])
#       } else {
#         new.start.map <- df$start.map[idx] # no change
#       }
#       
#       next.extent <- idx + 1
#       if (next.extent < nrow(df) && is.na(df$cn[next.extent]) && df$end.map[next.extent]-df$start.map[next.extent] < max.join*4) { next.extent <- next.extent + 1 }
#       if (next.extent <= nrow(df) && !is.na(df$cn[next.extent])) {
#         # half-way between current end and start of next extent
#         new.end.map <- df$end.map[idx] + (df$start.map[next.extent]-df$end.map[idx])/2
#         new.end.map <- ml.transition2(sample, df$end.map[idx], df$start.map[next.extent], df$cn[idx], df$cn[next.extent])
#       } else {
#         new.end.map <- df$end.map[idx] # no change
#       }
#     }
#     return(data.frame(.id=df$.id[idx], new.start.map=new.start.map, new.end.map=new.end.map,seg=df$seg[idx]))
#   })
#   return(new.bounds)
# })
# 
# # Create a new version of df and save it. Use same variable name so we can swap between the two
# # by just choosing a different file name.
# old.csm <- cn.segs.merged
# cn.segs.merged$start.map <- as.integer(new.cn.segs.merged$new.start.map)
# cn.segs.merged$end.map <- as.integer(new.cn.segs.merged$new.end.map)
# 
# save(cn.segs.merged, file=sprintf("%s.mlcsm.Rdata",cnv.seg.fn))
# write.table(select(cn.segs.merged, .id, seg, chr, start.map, end.map, copy.number), file=sprintf("%s.ml.tbl",cnv.seg.fn), sep="\t", row.names=FALSE, col.names=FALSE, quote=FALSE)
# write.table(cn.segs.merged[!is.na(cn.segs.merged$cn),c('chr','start.map','end.map','label','cn')], file=sprintf("%s.mlmrg.bed",cnv.seg.fn), sep="\t", col.names=FALSE, quote=FALSE, row.names=FALSE)
# 
# 
